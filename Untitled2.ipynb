{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "page_num = 1\n",
    "URL = 'http://news.chosun.com/svc/list_in/list.html?catid=2&pn=1'\n",
    "output_file = 'C:/feb/test1.txt'\n",
    "URL_with_page_num = URL \n",
    "\n",
    "    \n",
    "def get_text(URL, output_file):\n",
    "    source_code_from_url = urllib.request.urlopen(URL)\n",
    "    soup = BeautifulSoup(source_code_from_url, 'html.parser', from_encoding='utf-8')\n",
    "    content_of_article = soup.select('div.text')\n",
    "    for item in content_of_article:\n",
    "        string_item = str(item.find_all(text=True))\n",
    "        output_file.write(string_item)\n",
    "\n",
    "\n",
    "source_code_from_URL = urllib.request.urlopen(URL_with_page_num)\n",
    "soup = BeautifulSoup(source_code_from_URL,'html.parser',from_encoding='utf-8')\n",
    "for item in soup.select('dt > a'):\n",
    "    article_URL = item['href']\n",
    "    get_text(article_URL, output_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "\n",
    "def link_from_news_title(page_num,URL,output_file):\n",
    "    for i in range(page_num):\n",
    "        URL_with_page_num = URL + str(i)\n",
    "        source_code_from_URL = urllib.request.urlopen(URL_with_page_num)\n",
    "        soup = BeautifulSoup(source_code_from_URL, 'lxml',\n",
    "                             from_encoding='utf-8')\n",
    "        \n",
    "        for item in\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링을 시작합니다.\n",
      "rss 추출이 완료되었습니다.\n",
      "크롤링을 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 기사의 링크들이 담기는 리스트입니다.\n",
    "rsss = []\n",
    "\n",
    "# rss와 기사에서 특정 부분을 크롤링하는 함수입니다.\n",
    "def crawler(url, parser, css_selector):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, parser)\n",
    "    datas = soup.select(css_selector)\n",
    "    if parser == \"html.parser\":\n",
    "        print(datas[0].text)\n",
    "    else:\n",
    "        for data in datas:\n",
    "            rsss.append(data.text)\n",
    "\n",
    "#실행코드\n",
    "print(\"크롤링을 시작합니다.\")\n",
    "crawler('http://news.chosun.com/svc/list_in/list.html?catid=2&pn=4', 'html', 'item link')\n",
    "print(\"rss 추출이 완료되었습니다.\")\n",
    "\n",
    "for link in rsss:\n",
    "    try:\n",
    "        crawler(link, 'html.parser', '#article_body')\n",
    "        print(\"=\"*20)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Continuing ...')\n",
    "        continue\n",
    "\n",
    "print(\"크롤링을 종료합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n>>> f = open('your_file', 'a')\\n>>> f.write('\\n')\\n>>> f.close()\\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "https://yoonpunk.tistory.com/6\n",
    "\n",
    "**페이지 순환\n",
    "\n",
    "********final***********\n",
    "'''\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "\n",
    "\n",
    "\n",
    "page_num = 1\n",
    "\n",
    "output_file_name = 'C:/feb/test1.txt'\n",
    "\n",
    "\n",
    "URL_with_page_num = 'http://news.chosun.com/svc/list_in/list.html?catid=2&pn=4'\n",
    "source_code_from_URL = urllib.request.urlopen(URL_with_page_num)\n",
    "soup = BeautifulSoup(source_code_from_URL, 'html.parser')\n",
    "soup.encoding = 'utf-8' \n",
    "\n",
    "def get_text(URL, output_file):\n",
    "    source_code_from_url = urllib.request.urlopen(URL)\n",
    "    # urlllib로 기사 페이지를 요청받습니다.\n",
    "    soup = BeautifulSoup(source_code_from_url, 'html.parser')\n",
    "    soup.encoding = 'utf-8' \n",
    "    # BeautifulSoup로 페이지를 분석하기위해 soup변수로 할당 받습니다.\n",
    "    content_of_article = soup.select('#news_body_id > div')\n",
    "    # 기사의 본문내용을 추출\n",
    "    for item in content_of_article:\n",
    "        string_item = str(item.find_all(text=True))\n",
    "        output_file.write(string_item)                          #\n",
    "         \n",
    "i = 0\n",
    "\n",
    "    \n",
    "    for title in soup.find_all('dt'):\n",
    "        title_link = title.select('a')\n",
    "        article_URL = title_link[0]['href']\n",
    "        output_file_name = 'C:/feb/testfinal.txt'                 #\n",
    "        output_file = open(output_file_name, 'w', -1,\"utf-8\")       # for문 전에 \n",
    "        get_text(article_URL, output_file)\n",
    "        i +=1\n",
    "    output_file.close()                                        # for문 밖에서 (shift+tab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "https://yoonpunk.tistory.com/6\n",
    "\n",
    "**페이지 순환이전 \n",
    "\n",
    "********real final***********\n",
    "'''\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "\n",
    "\n",
    "\n",
    "URL_with_page_num = 'http://news.chosun.com/svc/list_in/list.html?catid=2&pn=1'\n",
    "source_code_from_URL = urllib.request.urlopen(URL_with_page_num)\n",
    "soup = BeautifulSoup(source_code_from_URL, 'html.parser')\n",
    "soup.encoding = 'utf-8' \n",
    "\n",
    "def get_text(URL, output_file):\n",
    "    source_code_from_url = urllib.request.urlopen(URL)\n",
    "    # urlllib로 기사 페이지를 요청받습니다.\n",
    "    soup = BeautifulSoup(source_code_from_url, 'html.parser')\n",
    "    soup.encoding = 'utf-8' \n",
    "    # BeautifulSoup로 페이지를 분석하기위해 soup변수로 할당 받습니다.\n",
    "    content_of_article = soup.select('#news_body_id > div')\n",
    "    # 기사의 본문내용을 추출\n",
    "    for item in content_of_article:\n",
    "        string_item = str(item.find_all(text=True))\n",
    "        output_file.write(string_item)                          #\n",
    "\n",
    "i = 0\n",
    "output_file_name = 'C:/feb/final.txt' \n",
    "output_file = open(output_file_name, 'w', -1,\"utf-8\")  # for문 전에 \n",
    "\n",
    "for title in soup.find_all('dt'):\n",
    "    title_link = title.select('a')\n",
    "    article_URL = title_link[0]['href']\n",
    "    get_text(article_URL, output_file)\n",
    "    i +=1\n",
    "    \n",
    "output_file.close()                                        # for문 밖에서 (shift+tab)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://yoonpunk.tistory.com/6\n",
    "\n",
    "**조선일보, 카테고리,페이지수입력후 기사 full내용 크롤링하는 함수**\n",
    "*remain : html.parser 대신 lxml\n",
    "********real final 2019-03-06*********\n",
    "[catogory]\n",
    "1경제             7전국\n",
    "2정치             8스포츠\n",
    "3사회             9연예\n",
    "4국제\n",
    "5문화\n",
    "6 오피니언\n",
    "'''\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "\n",
    "\n",
    "def get_text(URL, output_file):\n",
    "    source_code_from_url = urllib.request.urlopen(URL)\n",
    "    # urlllib로 기사 페이지를 요청받습니다.\n",
    "    soup = BeautifulSoup(source_code_from_url, 'html.parser')\n",
    "    soup.encoding = 'utf-8' \n",
    "    # BeautifulSoup로 페이지를 분석하기위해 soup변수로 할당 받습니다.\n",
    "    content_of_article = soup.select('#news_body_id > div')\n",
    "    # 기사의 본문내용을 추출\n",
    "    for item in content_of_article:\n",
    "        string_item = str(item.find_all(text=True))\n",
    "        output_file.write(string_item)                          #\n",
    "\n",
    "##########################################################################################3\n",
    "def crawlling(page,category):\n",
    "    page = int(page)\n",
    "    \n",
    "    for page in range(1,page+1):\n",
    "        URL_with_page_num = 'http://news.chosun.com/svc/list_in/list.html?catid='+str(category)+'&pn='+str(page)\n",
    "        source_code_from_URL = urllib.request.urlopen(URL_with_page_num)\n",
    "        soup = BeautifulSoup(source_code_from_URL, 'html.parser')\n",
    "        soup.encoding = 'utf-8' \n",
    "\n",
    "\n",
    "        i = 0\n",
    "        output_file_name = 'C:/feb/{category}_page{page}.txt'.format(category=category,page=page) \n",
    "        output_file = open(output_file_name, 'w', -1,\"utf-8\")  # for문 전에 \n",
    "\n",
    "        for title in soup.find_all('dt'):\n",
    "            title_link = title.select('a')\n",
    "            article_URL = title_link[0]['href']\n",
    "            get_text(article_URL, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://yoonpunk.tistory.com/6\n",
    "\n",
    "**조선일보, 카테고리,페이지수입력후 기사 full내용 크롤링하는 함수**\n",
    "*remain : html.parser 대신 lxml\n",
    "********real final 2019-03-06*********\n",
    "[catogory]\n",
    "1경제             7전국\n",
    "2정치             8스포츠\n",
    "3사회             9연예\n",
    "4국제\n",
    "5문화\n",
    "6 오피니언\n",
    "'''\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "\n",
    "\n",
    "def get_text(URL, output_file):\n",
    "    source_code_from_url = urllib.request.urlopen(URL)\n",
    "    # urlllib로 기사 페이지를 요청받습니다.\n",
    "    soup = BeautifulSoup(source_code_from_url, 'html.parser')\n",
    "    soup.encoding = 'utf-8' \n",
    "    # BeautifulSoup로 페이지를 분석하기위해 soup변수로 할당 받습니다.\n",
    "    content_of_article = soup.select('#news_body_id > div')\n",
    "    # 기사의 본문내용을 추출\n",
    "    for item in content_of_article:\n",
    "        string_item = str(item.find_all(text=True))\n",
    "        output_file.write(string_item)                          #\n",
    "\n",
    "##########################################################################################3\n",
    "def crawlling(page,category):\n",
    "    page = int(page)\n",
    "    \n",
    "    for page in range(1,page+1):\n",
    "        URL_with_page_num = 'http://news.chosun.com/svc/list_in/list.html?catid='+str(category)+'&pn='+str(page)\n",
    "        source_code_from_URL = urllib.request.urlopen(URL_with_page_num)\n",
    "        soup = BeautifulSoup(source_code_from_URL, 'html.parser')\n",
    "        soup.encoding = 'utf-8' \n",
    "\n",
    "\n",
    "        i = 0\n",
    "        output_file_name = 'C:/feb/{category}_page{page}.txt'.format(category=category,page=page) \n",
    "        output_file = open(output_file_name, 'w', -1,\"utf-8\")  # for문 전에 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "https://yoonpunk.tistory.com/6\n",
    "\n",
    "**조선일보, 카테고리,페이지수입력후 기사 full내용 크롤링하는 함수**\n",
    "*remain : html.parser 대신 lxml\n",
    "********real final 2019-03-06*********\n",
    "[catogory]\n",
    "1경제             7전국\n",
    "2정치             8스포츠\n",
    "3사회             9연예\n",
    "4국제\n",
    "5문화\n",
    "6 오피니언\n",
    "'''\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "\n",
    "\n",
    "def get_text(URL, output_file):\n",
    "    source_code_from_url = urllib.request.urlopen(URL)\n",
    "    # urlllib로 기사 페이지를 요청받습니다.\n",
    "    soup = BeautifulSoup(source_code_from_url, 'html.parser')\n",
    "    soup.encoding = 'utf-8' \n",
    "    # BeautifulSoup로 페이지를 분석하기위해 soup변수로 할당 받습니다.\n",
    "    content_of_article = soup.select('#news_body_id > div')\n",
    "    # 기사의 본문내용을 추출\n",
    "    for item in content_of_article:\n",
    "        string_item = str(item.find_all(text=True))\n",
    "        output_file.write(string_item)                          #\n",
    "\n",
    "##########################################################################################3\n",
    "def crawlling(page,category):\n",
    "    page = int(page)\n",
    "    \n",
    "    for page in range(1,page+1):\n",
    "        URL_with_page_num = 'http://news.chosun.com/svc/list_in/list.html?catid='+str(category)+'&pn='+str(page)\n",
    "        source_code_from_URL = urllib.request.urlopen(URL_with_page_num)\n",
    "        soup = BeautifulSoup(source_code_from_URL, 'html.parser')\n",
    "        soup.encoding = 'utf-8' \n",
    "\n",
    "\n",
    "        i = 0\n",
    "        output_file_name = 'C:/feb/{category}_page{page}.txt'.format(category=category,page=page) \n",
    "        output_file = open(output_file_name, 'w', -1,\"utf-8\")  # for문 전에 \n",
    "\n",
    "        for title in soup.find_all('dt'):\n",
    "            title_link = title.select('a')\n",
    "            article_URL = title_link[0]['href']\n",
    "            get_text(article_URL, output_file)\n",
    "            i +=1\n",
    "\n",
    "        output_file.close()                                        # for문 밖에서 (shift+tab)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "crawlling(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/feb/final1.txt\n",
      "C:/feb/final2.txt\n",
      "C:/feb/final3.txt\n",
      "C:/feb/final4.txt\n",
      "C:/feb/final5.txt\n",
      "C:/feb/final6.txt\n",
      "C:/feb/final7.txt\n",
      "C:/feb/final8.txt\n",
      "C:/feb/final9.txt\n",
      "C:/feb/final10.txt\n"
     ]
    }
   ],
   "source": [
    "for page in range(1,11):\n",
    "    print('C:/feb/final%d.txt' % page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "\n",
    "def get_link_from_news_title(URL,output_file):\n",
    "    for i in range(page_num):\n",
    "        current_page_num = 1 + i*15\n",
    "        # 페이지당 15개의 게시물.        \n",
    "     \n",
    "        # URL 처음 = 오는 위치 반환 (URL에 몇페이지 인지 추가하기 위해)        \n",
    "         # 페이지가 있는 URL 재구성        \n",
    "        source_code_from_URL=urllib.request.urlopen(URL)\n",
    "        # 재구성한 URL을 request로 호출        \n",
    "        soup=BeautifulSoup(source_code_from_URL,  'html.parser')\n",
    "        soup.encoding = 'utf-8' \n",
    "        # BeautifulSoup로 변환, 기사 분석 후 추출하기 위해\n",
    "        \n",
    "        for title in soup.find_all('dt'):\n",
    "            title_link = title.select('a')\n",
    "            article_URL = title_link[0]['href']\n",
    "            get_text(article_URL, output_file)\n",
    "        # 본문 기사가 담긴 URL을 찾기위해\n",
    "def get_text(URL, output_file):\n",
    "    source_code_from_url = urllib.request.urlopen(URL)\n",
    "    # urlllib로 기사 페이지를 요청받습니다.\n",
    "    soup = BeautifulSoup(source_code_from_url, 'html.parser')\n",
    "    soup.encoding = 'utf-8' \n",
    "    # BeautifulSoup로 페이지를 분석하기위해 soup변수로 할당 받습니다.\n",
    "    content_of_article = soup.select('div.article_txt')\n",
    "    # 기사의 본문내용을 추출\n",
    "    for item in content_of_article:\n",
    "        string_item = str(item.find_all(text=True))\n",
    "        output_file.write(string_item)\n",
    "    # 기사 텍스트가 있다면 파일에 쓴다\n",
    "    \n",
    "def main():\n",
    "    output_file_name = \"out.txt\" #출력할 파일명\n",
    "    target_URL = 'http://news.chosun.com/svc/list_in/list.html?catid=2&pn=4'\n",
    "    output_file = open(output_file_name, 'w',-1,\"utf-8\")\n",
    "    get_link_from_news_title(target_URL, output_file)\n",
    "    output_file.close()\n",
    " \n",
    " \n",
    "if __name__== '__main__':\n",
    "  output_file  main()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1,5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
